{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from image_preprocessing import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# import pandas as pd\n",
    "import pickle\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "from Feature_Extraction import *\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_images(directory, debug = False):\n",
    "    list_target_names = []\n",
    "    list_images = []\n",
    "\n",
    "    for path, subdirs, files in os.walk(directory):\n",
    "        # if(path.startswith(directory + '.')):\n",
    "        #     continue\n",
    "        # files = [f for f in files if not f[0] == '.'] # Ignore '.directory' file\n",
    "        if debug:\n",
    "            print(\"path = \", path, \" number of images = \", len(files))\n",
    "\n",
    "        for name in files:\n",
    "            # image=cv2.imread(os.path.join(path, name))\n",
    "\n",
    "            # image = cv2.imread(os.path.join(path, name), cv2.IMREAD_GRAYSCALE)\n",
    "            # result=cv2.resize(image, (128, 64)) # multiply by 4\n",
    "\n",
    "            # pre processing on the image (madbouly)\n",
    "            image = Image.open(os.path.join(path, name)).convert('RGB')\n",
    "            binary, result = image_pre_processing(image)\n",
    "            result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            if debug:\n",
    "                print(\"image name = \", name)\n",
    "                cv2.imshow(\"Image\", image)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "            list_target_names.append(os.path.basename(path))\n",
    "            list_images.append(result)\n",
    "\n",
    "    return list_target_names,  list_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_feature_size(list, maxSize):\n",
    "    feature_vector = np.asarray(list).flatten()\n",
    "    max_size = min(maxSize,len(feature_vector)) \n",
    "    features = np.zeros((maxSize,))\n",
    "    features[0:max_size] = feature_vector[0:max_size]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extraction(images):\n",
    "    list = []\n",
    "    maxSize = 3000\n",
    "    # list = np.array([hog.compute(image)  for image in images])\n",
    "    all_size = 0\n",
    "    for image in images:\n",
    "        # kp, features_list = orb.detectAndCompute(image, None)\n",
    "        # features_list =lbp(image, radius=3, n_points=8)\n",
    "        features = hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2))\n",
    "        # shi = shiThomasFeatureExtraction(image, 100, 0.01, 10)\n",
    "        # kp, features_list = SIFT_features(image)\n",
    "\n",
    "        \n",
    "        # feature_vector = features_list.flatten()\n",
    "        # features = fixed_feature_size(shi, maxSize)\n",
    "        # features_list = fixed_feature_size(features_list, maxSize)\n",
    "\n",
    "        # list.append(np.concatenate((features, features_list), axis = None))\n",
    "        # list.append(shi)\n",
    "        list.append(features)\n",
    "        # features_list, Hog_img = hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2))\n",
    "        # list.append(features_list)\n",
    "    # x = min(list, key=len)\n",
    "    # x = len(x)\n",
    "    # features = []\n",
    "    # for z in list:\n",
    "    #     features.append(z[:x])\n",
    "    list = np.asarray(list)\n",
    "    return list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning_classifier(directory):\n",
    "\n",
    "    target_names, images = obtain_images(directory)\n",
    "    target_names_shuffled, images_shuffled = shuffle(np.array(target_names), np.array(images))  # reorder el array bas\n",
    "\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(images_shuffled, target_names_shuffled, random_state=0, test_size=0.3)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(Xtest, ytest, test_size=0.5, random_state=0)\n",
    "\n",
    "    print(len(X_val))\n",
    "\n",
    "    X_val = features_extraction(X_val)\n",
    "\n",
    "    return y_val, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tunning_feature_extraction(directory):\n",
    "\n",
    "    target_names, images = obtain_images(directory)\n",
    "    y, X = shuffle(np.array(target_names), np.array(images))  # reorder el array bas\n",
    "\n",
    "    # x = features_extraction(x)\n",
    "\n",
    "    return y, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")\n",
    "y, X = tunning_classifier(directory='./Dataset/')\n",
    "print(\"hello2\")\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10],\n",
    "              'kernel': ['rbf', 'poly'],\n",
    "              'degree': [2, 3, 4],\n",
    "              'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
    "              'coef0': [-1, 0, 1]}\n",
    "\n",
    "# Create an SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Perform a grid search\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
